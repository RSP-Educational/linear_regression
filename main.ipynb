{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ea8b615",
   "metadata": {},
   "source": [
    "<h1>Coding Session #1 - Lineare Regression</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05aea99",
   "metadata": {},
   "source": [
    "## 1 Vorbereitung\n",
    "\n",
    "Bei dieser Datei (`*.ipynb`) handelt sich um ein Jupyter Notebook. Das ist eine interaktive Datei, in der neben strukturierten Textzellen (`markdown`) Codezellen direkt integriert und ausgeführt werden können. Die Verwendung von `Visual Studio Code` wird empfohlen.\n",
    "\n",
    "### 1.1 Jupyter Extension installieren\n",
    "1. Klicken Sie auf Extensions<br>\n",
    "    <img src=\"images/vscode_extensions.png\" width=\"300px\"></img>\n",
    "2. Suchen Sie nach \"Jupyter\" und installieren Sie die Erweiterung.<br>\n",
    "    <img src=\"images/install_jupyter.png\" width=\"500px\"></img>\n",
    "\n",
    "### 1.2 Virtuelle Umgebung\n",
    "\n",
    "**Virtuelle Umgebungen** sorgen dafür, dass Paketinstallationen nur im lokalen Projektkontext (eben im virtuellen Environment) erfolgen. So werden Versionskonflikte mit bestehenden Installationen vermieden.\n",
    "\n",
    "**1.** Führen Sie im Terminal folgende Codezelle aus, um eine virtuelle Umgebung zu erstellen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b1541a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m venv .venv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b838657",
   "metadata": {},
   "source": [
    "**2.** Führen Sie je nach Betriebssystem eine der folgenden Zellen aus:\n",
    "\n",
    "Windows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9907c60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!.\\.venv\\Scripts\\Activate.ps1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175db468",
   "metadata": {},
   "source": [
    "Linux & MacOS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26963518",
   "metadata": {},
   "outputs": [],
   "source": [
    "!source .venv/bin/activate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1154535a",
   "metadata": {},
   "source": [
    "**3.1** Klicken Sie oben rechts in Visual Studio Code auf die Python Version<br>\n",
    "**3.2** Klicken Sie auf _Anderen Kernel auswählen..._\n",
    "   \n",
    "<img src=\"images/prepare001.png\" width=\"800px\"></img>\n",
    "\n",
    "**4.** Klicken Sie auf _Python Umgebungen..._\n",
    "   \n",
    "<img src=\"images/prepare002.png\" width=\"500px\"></img>\n",
    "\n",
    "**3.** Wählen Sie _.venv (Python 3.12.0)_\n",
    "\n",
    "<img src=\"images/prepare003.png\" width=\"500px\"></img>\n",
    "\n",
    "### 1.3 Requirements installieren\n",
    "\n",
    "Zur Vorbereitung stellen Sie bitte sicher, dass alle benötigten Bibliotheken installiert sind, indem Sie im folgende Zelle ausführen:<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5921f048",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb38e59c",
   "metadata": {},
   "source": [
    "## 2 Motivation\n",
    "\n",
    "Künstliche Neuronale Netze bestehen aus einer Vielzahl an Neuronen. Jedes Neuron ist in der Lage, eine simple mathematische Repräsentation zu lernen. Durch Verknüpfung mehrerer Neuronen können komplexe mathematische Funktionen nachgebildet werden. So kommen neuronale Netze in bspw. in der Bildverarbeitung, der Spracherkennung, für Wetter-, Energiebedarfs- oder Verschleißsprognosen zum Einsatz.\n",
    "\n",
    "## 3 Ziel\n",
    "\n",
    "Das Grundprinzip eines jeden Neurons ist **lineare Regression**. In diesem Coding Beispiel wird vorerst die Funktionsweise eines einzelnen Neurons betrachtet, bevor mehrere Neuronen zu einem neuronalen Netz verknüpft werden, um dessen Funktionsweise zu verdeutlichen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f51aef",
   "metadata": {},
   "source": [
    "## 4 Code\n",
    "\n",
    "### 4.1 Daten Generierung\n",
    "\n",
    "Zuerst wird die Bibliothek `Numpy` importiert, welche fundamentale Funktionalitäten für numerische Berechnungen in Python bereitstellt.\n",
    "\n",
    "Anschließend werden Beispieldaten generiert. Dafür wird die vorgefertigte Funktion `generate_linear_data` aus der Datei `utilities/data.py` importiert und aufgerufen. Diese generiert Daten, die einer linearen Funktion $y=m\\cdot x+n$ folgen, wobei `m` der Anstieg, `n` die y-Verschiebung und `num_samples` die Anzahl der generierten Datenpunkte ist.\n",
    "\n",
    "Die vorgefertigten Funktionen `plot_data_points` und `plot_series` werden aus der Datei `utilities/visualization.py` importiert. Diese dienen der Visualisierung der Daten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b019cfb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                                                  # Numpy is a fundamental package for numerical computations in Python\n",
    "from utilities.data import generate_linear_data                     # Custom module for generating linear data\n",
    "from utilities.visualization import plot_data_points, plot_series   # Custom module for visualizing 2D \n",
    "\n",
    "X, Y = generate_linear_data(\n",
    "    m           = 1,\n",
    "    n           = 0,\n",
    "    num_samples = 20\n",
    ")\n",
    "plot_data_points(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7603d7",
   "metadata": {},
   "source": [
    "Der Parameter `noise` fügt zufällig künstlich generiertes Rauschen zu den Daten hinzu. Da reale Daten meist nicht perfekt sind, wird dieses Rauschen hier durch Zufallswerte simuliert:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8220fd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = generate_linear_data(\n",
    "    m           = 1.0,\n",
    "    n           = 0.0,\n",
    "    num_samples = 50,\n",
    "    noise       = 0.15\n",
    ")\n",
    "plot_data_points(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99d90f8",
   "metadata": {},
   "source": [
    "### 4.2 Lineare Regression\n",
    "\n",
    "Das Ziel neuronaler Netze ist es, eine Funktion zu finden, die die Daten möglichst gut repräsentiert. Wir starten hier eine einfache lineare Abhängigkeit durch eine lineare Regression zu approximieren. Das ist genau das, was in einem einzelnen Neuron passiert.\n",
    "\n",
    "#### 4.2.1 Daten generieren\n",
    "\n",
    "Zuerst wird der Datensatz generiert $D=(X,Y)$:\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;$X$ ...Eingabedaten<br>\n",
    "&nbsp;&nbsp;&nbsp;$Y$ ...Zieldaten $\\to$ es soll gelernt werden, diese Daten in Abhängigkeit von $X$ vorherzusagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb0b925",
   "metadata": {},
   "outputs": [],
   "source": [
    "m           = 0.7   # Anstieg der Geraden\n",
    "n           = 1.5   # y-Verschiebung der Geraden\n",
    "noise       = 0.1   # Rauschanteil\n",
    "\n",
    "X, Y = generate_linear_data(\n",
    "    num_samples     = 50,\n",
    "    m               = m,\n",
    "    n               = n,\n",
    "    noise           = noise\n",
    ")\n",
    "\n",
    "plot_data_points(input=X, target=Y, prediction=None, title=f\"Linear Data with noise\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439df3c6",
   "metadata": {},
   "source": [
    "#### 4.2.2 Stochastic Gradient Descent (SGD)\n",
    "\n",
    "Nun versuchen wir die Funktion $\\hat{y}=w\\cdot x+b$ so zu optimieren, dass sie die Daten möglichst gut abbildet. Dafür werden mittels **Gradient Descent Algorithmus** die freien Parameter $w$ und $b$ iterativ angepasst.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;$x$ ...x-Wert aus den Daten (`inputs`)<br>\n",
    "&nbsp;&nbsp;&nbsp;$y$ ...y-Wert aus den Daten (`targets`)<br>\n",
    "&nbsp;&nbsp;&nbsp;$w$ ...weight (zu lernender Anstieg, unbekannt)<br>\n",
    "&nbsp;&nbsp;&nbsp;$b$ ...bias (zu lernende y-Verschiebung, unbekannt)\n",
    "\n",
    "> ---\n",
    "> **Gradient Descent Algorithmus**\n",
    "> \n",
    "> ---\n",
    ">\n",
    "> **Eingaben:**\n",
    "> - Lernrate $\\eta$ (eta)\n",
    "> - Trainingsdaten $D = {(X,Y)}$\n",
    "> - Anzahl Epochen $E$\n",
    "> - Initiales weight $w\\leftarrow 0$\n",
    "> - Initialer Bias $b\\leftarrow 0$\n",
    ">\n",
    "> **Ausgaben**\n",
    "> - Vorhersage $\\hat{Y}$\n",
    "> \n",
    "> **for** $epoch = 1,\\,...,\\,E$ **do**<br>\n",
    ">> **for** $x_i,y_i\\in D$ **do**<br>\n",
    ">>> $\\hat{y_i} = w\\cdot x_i + b$<br>\n",
    ">>> $dw = 2 \\cdot (y_i-\\hat{y_i}) \\cdot x_i$<br>\n",
    ">>> $db = 2 \\cdot (y_i-\\hat{y_i})$\n",
    ">>>\n",
    ">>> $w\\leftarrow w - \\eta \\cdot dw$<br>\n",
    ">>> $b\\leftarrow b - \\eta \\cdot db$\n",
    ">>\n",
    ">> **end for**\n",
    ">\n",
    "> **end for**\n",
    ">\n",
    "> ---\n",
    "\n",
    "**Erklärung**\n",
    "\n",
    "Die lineare Regression minimiert die Summe der quadrierten Abweichungen zwischen tatsächlichen Werten $\\hat{y}$ und vorhergesagten Werten $y$, indem sie den quadratischen Fehler (Mean Square Error - MSE) minimiert.\n",
    "$$L_{MSE}=(y-\\hat{y})^2$$\n",
    "$$L_{MSE}=(y-(wx+b))^2$$\n",
    "\n",
    "Man startet mit zufälligen Parametern weight $w$ und bias $b$ und verbessert sie in kleinen Schritten, indem man die partielle Ableitung der Loss-Funktion nach $w$ und $b$ bildet und diese multipliziert mit der Lernrate $\\eta$ auf $w$ bzw. $b$ addiert:\n",
    "$$w \\leftarrow w - \\eta \\frac{\\partial Loss}{\\partial w}$$\n",
    "$$b \\leftarrow b - \\eta \\frac{\\partial Loss}{\\partial b}$$\n",
    "\n",
    "Die partiellen Ableitungen der MSE-Loss-Funktion sind hierbei\n",
    "$$\\frac{\\partial L_{MSE}}{\\partial w}=2\\cdot(wx+b-y)\\cdot x=2\\cdot(\\hat{y}-y)\\cdot x$$\n",
    "$$\\frac{\\partial{L_{MSE}}}{\\partial{b}}=2\\cdot(wx+b-y)=2\\cdot(\\hat{y}-y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f0eb02",
   "metadata": {},
   "source": [
    "> <span style=\"color:#00A1E3\">**Aufgabe 1 - Stochastic Gradient Decsent**</span>\n",
    ">\n",
    "> Implementieren Sie den Stohastic Gradient Descent (SGD) Algorithmus für ein einzelnes Neuron. Finden Sie geeignete Parameter für die Anzahl der Epochen sowie die Lernrate. Nutzen Sie für das Gewicht `w` und den Bias `b` skalare Werte, initialisieren Sie diese mit `0.0`.\n",
    ">\n",
    "> Iterieren Sie in jeder Epoche über jeden einzelnen Datenpunkt für die _Forward Propagation_ und die _Backpropagation_.\n",
    ">\n",
    "> Nutzen Sie den Mean Squared Error (MSE).\n",
    ">\n",
    "> Speichern Sie die vorhergesagten Werte `y_hat` für jede Epoche in einer Liste `Y_hat`, welche am Anfang jeder Epoche mit `Y_hat = []` initialisiert wird.\n",
    ">\n",
    "> Fügen Sie am Ende der Epoche folgendes Code-Segment hinzu, um den Trainingsfortschritt zu visualisieren:\n",
    ">\n",
    "> ```python\n",
    ">    losses.append(loss)\n",
    ">\n",
    ">    if (epoch+1) % 20 == 0 or epoch == 0 or epoch == EPOCHS - 1:\n",
    ">        plot_data_points(input=X, target=Y, prediction=Y_hat, title=f\"Linear Data Fitting - Epoch {epoch+1}\")\n",
    ">        print(f\"Epoch {epoch+1}/{EPOCHS}, MSE: {loss:0.6f}\")\n",
    "> ```\n",
    "> <br>\n",
    ">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f1bf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Initialize parameters\n",
    "EPOCHS      =           # Number of training epochs\n",
    "LR          =           # Learning Rate\n",
    "w, b        =           # weight and bias\n",
    "losses      = []        # to store loss values\n",
    "\n",
    "# TODO: Training loop\n",
    "    \n",
    "\n",
    "plot_series(data=losses, title=\"Training Loss over Epochs\", xlabel=\"Epochs\", ylabel=\"MSE Loss\")\n",
    "print(f'Learned parameters: w = {w.flatten()[0]:.4f}, b = {b.flatten()[0]:.4f}, True parameters: m = {m}, n = {n}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f96ecfd",
   "metadata": {},
   "source": [
    "#### 4.2.3 Mini-Batch Training\n",
    "\n",
    "Im Mini-Batch Training werden die Trainingsdaten in kleinere Teilmengen unterteilt (Mini-Batches).\n",
    "\n",
    "<img src=\"images/mini_batch.png\" width=\"700px\">\n",
    "\n",
    "Mini-Batches werden zufällig aus dem Datensatz gesampelt. Das Training läuft iterativ für für jedes Mini-Batch ab.  Die durchschnittlichen Gradienten für jedes Mini-Batch werden einzeln zurückpropagiert.\n",
    "\n",
    "Warum Mini-Batch Training?\n",
    "- Optimiert Speichernutzung\n",
    "- Beschleunigt Trainingsprozess →  Daten können parallel verarbeitet werden\n",
    "- Verbessert die Modellkonvergenz durch stabilere Gradientenupdates (weniger Rauschen)\n",
    "- Bessere Generalisierung → Vermeidung lokaler Minima\n",
    "\n",
    "Typische Batch-Größen: 8, 16, 32, 64, 128\n",
    "- Wahl der Batch-Größe ist ein wichtiger Hyperparameter\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1287424f",
   "metadata": {},
   "source": [
    "> ---\n",
    "> **Mini-Batch Gradient Descent Algorithmus**\n",
    "> \n",
    "> ---\n",
    "> \n",
    "> **Eingaben:**\n",
    "> - Lernrate $\\eta$ (eta)\n",
    "> - Trainingsdaten $D = (X,Y)$\n",
    "> - Anzahl Epochen $E$\n",
    "> - Batch-Größe $B$\n",
    "> - Initiales weight $w\\leftarrow 0$\n",
    "> - Initialer Bias $b\\leftarrow 0$\n",
    "> \n",
    "> **Ausgaben:**\n",
    "> - Vorhersage Ŷ\n",
    "> \n",
    "> **for** $epoch = 1,\\,...,\\,E$ **do**\n",
    ">> $D_{shuffled}=shuffle(D)$\n",
    ">>     \n",
    ">> **for** $b\\leftarrow 0$ **to** $length(D)$ **step** $B$ **do**\n",
    ">>> $batch=D_{shuffled}[b\\,\\,to\\,\\,b + B]$\n",
    ">>> \n",
    ">>> $\\Delta w=\\frac{1}{B}\\sum_{i=1}^{B}{2\\cdot(y_i-\\hat{y}_i)\\cdot x_i}$<br>\n",
    ">>> $\\Delta b=\\frac{1}{B}\\sum_{i=1}^{B}{2\\cdot(y_i-\\hat{y}_i)}$\n",
    ">>>\n",
    ">>> $w\\leftarrow w-\\eta\\cdot \\Delta w$<br>\n",
    ">>> $b\\leftarrow b-\\eta\\cdot \\Delta b$\n",
    ">>>     \n",
    ">> **end for**\n",
    ">> \n",
    "> **end for**\n",
    "> \n",
    "> ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ebdf33",
   "metadata": {},
   "source": [
    "> <span style=\"color:#00A1E3\">**Aufgabe 1 - Stochastic Gradient Decsent**</span>\n",
    "> Implementieren Sie den Mini-Btch SGD Algorithmus.\n",
    ">\n",
    "> Fügen Sie am Ende jeder Epohe folgendes Code-Segment hinzu:\n",
    "> ```python\n",
    ">     # Compute predictions for all data for visualization\n",
    ">     Y_hat = w * X + b\n",
    ">     \n",
    ">     losses.append(loss)\n",
    ">     \n",
    ">     if (epoch+1) % 20 == 0 or epoch == 0 or epoch == EPOCHS - 1:\n",
    ">         plot_data_points(input=X, target=Y, prediction=Y_hat, title=f\"Linear Data Fitting (Mini-Batch) - Epoch {epoch+1}\")\n",
    ">         print(f\"Epoch {epoch+1}/{EPOCHS}, MSE: {loss:0.6f}, Batch Size: {BATCH_SIZE}\")\n",
    "> ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6407c576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Initialize parameters\n",
    "EPOCHS              =               # Number of training epochs\n",
    "LR                  =               # Learning Rate\n",
    "BATCH_SIZE          =               # Mini-batch size\n",
    "w, b                =               # weight and bias\n",
    "losses              = []            # to store loss values\n",
    "\n",
    "# TODO: Training loop\n",
    "\n",
    "\n",
    "\n",
    "plot_series(data=losses, title=\"Mini-Batch Loss over Epochs\", xlabel=\"Epochs\", ylabel=\"MSE Loss\")\n",
    "print(f'Learned parameters: w = {w:.4f}, b = {b:.4f}, True parameters: m = {m}, n = {n}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969200c8",
   "metadata": {},
   "source": [
    "Sie haben nun gelernt, wie ein einzelnes Neuron lernt. In Part 2 werden wir uns anschauen, wie dies in einem neuronalen Netz funktioniert."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
